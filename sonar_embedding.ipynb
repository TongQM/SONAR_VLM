{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongQM/SONAR_VLM/blob/main/sonar_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8SKAdaYm_3f"
      },
      "source": [
        "# Install SONAR embeddings and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqEBmIjJm5kP",
        "outputId": "0c3bda25-3eb0-4ab3-a12f-788ad8b1b639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.6.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: fairseq2n, sonar-space, torchaudio\n"
          ]
        }
      ],
      "source": [
        "# !pip show torch\n",
        "# !pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.6.0/cu124\n",
        "# !pip install sonar-space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VswdE45TnzjT"
      },
      "source": [
        "# Mount Google drive and set work directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRalEcpMn3DA",
        "outputId": "824d3896-9089-4604-ea9d-846207e7b5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.chdir('/content/gdrive/My Drive/sonar')\n",
        "# !ls annotations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet50, resnet152, ResNet152_Weights, ResNet50_Weights\n",
        "from torchsummary import summary\n",
        "\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = 'mps' if torch.backends.mps.is_available() else device\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFDQCp_p5iu"
      },
      "source": [
        "## Load text2embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentences:\n",
            "I want to get an intern offer pls\n",
            "Why I have so bad luck\n",
            "\n",
            "Reconstructed sentences:\n",
            "I want to get an internship offer pls\n",
            "Why I have such bad luck\n"
          ]
        }
      ],
      "source": [
        "t2vec_model = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\",\n",
        "                                           tokenizer=\"text_sonar_basic_encoder\",\n",
        "                                           device=torch.device('cpu'),\n",
        "                                           dtype=torch.float16)\n",
        "sentences = ['I want to get an intern offer pls', 'Why I have so bad luck']\n",
        "embeddings = t2vec_model.predict(sentences, source_lang=\"eng_Latn\")\n",
        "\n",
        "\n",
        "vec2text_model = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\",\n",
        "                                              tokenizer=\"text_sonar_basic_encoder\",\n",
        "                                              device=torch.device('cpu'),\n",
        "                                              dtype=torch.float16)\n",
        "reconstructed = vec2text_model.predict(embeddings, target_lang=\"eng_Latn\", max_seq_len=512)\n",
        "print(\"Original sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "print(\"\\nReconstructed sentences:\")\n",
        "for sentence in reconstructed:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgFB2wVJoZKE"
      },
      "source": [
        "## Load train and validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define dataloader with SONAR embeddings as labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {},
      "outputs": [],
      "source": [
        "class COCOCaptionTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns (image_tensor, list_of_captions) for each idx.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, coco_json, transform=None, numcaps=5, subset=1.0):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        with open(coco_json) as f:\n",
        "            coco = json.load(f)\n",
        "        # group captions by image_id\n",
        "        caps = defaultdict(list)\n",
        "        for ann in coco[\"annotations\"]:\n",
        "            caps[f\"{ann['image_id']:012d}.jpg\"].append(ann[\"caption\"])\n",
        "        \n",
        "        # For each image, sort the captions and keep only the first 5\n",
        "        for img, captions in caps.items():\n",
        "            caps[img] = sorted(captions)[:numcaps]\n",
        "        \n",
        "        self.samples = sorted((str(self.img_dir / img), caps[img]) \n",
        "                              for img in caps)\n",
        "        \n",
        "        # If subset < 1.0, take the first subset proportion sample of the dataset\n",
        "        if subset < 1.0:\n",
        "            total = len(self.samples)\n",
        "            k = max(1, int(total * subset))\n",
        "            self.samples = self.samples[:k]\n",
        "\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, captions = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, captions\n",
        "\n",
        "\n",
        "def collate_and_encode(batch, text_encoder, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    batch: list of (image_tensor, [cap1, cap2, ..., cap5])\n",
        "    text_encoder: any model with .encode(list[str]) â†’ Tensor(n_captions, D)\n",
        "    \"\"\"\n",
        "    # 1) Stack images\n",
        "    imgs = torch.stack([b[0] for b in batch], dim=0)\n",
        "\n",
        "    # 2) Flatten out all captions\n",
        "    all_caps = []\n",
        "    counts   = []\n",
        "    for _, caps in batch:\n",
        "        counts.append(len(caps))\n",
        "        all_caps.extend(caps)\n",
        "\n",
        "    # 3) Run teacher in one shot\n",
        "    #    Assume it returns a torch.Tensor of shape (sum(counts), D)\n",
        "    with torch.no_grad():\n",
        "        embs = text_encoder.predict(all_caps, source_lang=\"eng_Latn\").to(device)\n",
        "        # if it returns numpy, wrap: embs = torch.from_numpy(embs).to(device)\n",
        "\n",
        "    # 4) Split & reduce per sample (e.g. mean)\n",
        "    D = embs.size(1)\n",
        "    labels = []\n",
        "    idx = 0\n",
        "    for n in counts:\n",
        "        chunk = embs[idx: idx+n]      # shape (n, D)\n",
        "        labels.append(chunk)  # â†’ (D,)\n",
        "        idx += n\n",
        "    labels = torch.stack(labels, dim=0)  # â†’ (batch_size, D)\n",
        "\n",
        "    return imgs.to(device), labels\n",
        "\n",
        "\n",
        "# ---- usage ----\n",
        "train_dataset = COCOCaptionTextDataset(\n",
        "    img_dir=\"./data/images/train2017\",\n",
        "    coco_json=\"./data/annotations/annotations/captions_train2017.json\",\n",
        "    numcaps=5,\n",
        "    subset=0.1,  # 0.1 = 10% of the dataset\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: collate_and_encode(b, t2vec_model, device=\"mps\"),\n",
        ")\n",
        "\n",
        "val_dataset = COCOCaptionTextDataset(\n",
        "    img_dir=\"./data/images/val2017\",\n",
        "    coco_json=\"./data/annotations/annotations/captions_val2017.json\",\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: collate_and_encode(b, t2vec_model, device=\"mps\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 11828 training samples.\n",
            "There are 5000 validation samples.\n"
          ]
        }
      ],
      "source": [
        "print(f\"There are {len(train_dataset)} training samples.\")\n",
        "print(f\"There are {len(val_dataset)} validation samples.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define multi-positive InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_pos_infonce_loss(img_embs, cap_embs, temperature):\n",
        "    \"\"\"\n",
        "    img_embs: (B, D)  after F.normalize\n",
        "    cap_embs: (B, m, D) after F.normalize\n",
        "    \"\"\"\n",
        "    B, m, D = cap_embs.shape\n",
        "    # flatten captions: (B*m, D)\n",
        "    flat_caps = cap_embs.view(B*m, D)                             \n",
        "    # similarity: (B, B*m)\n",
        "    logits = img_embs @ flat_caps.t() / temperature               \n",
        "\n",
        "    # numerator: sum over each image's m positives\n",
        "    # we know that positives for image i are indices [i*m : i*m + m]\n",
        "    pos_mask = torch.zeros_like(logits, dtype=torch.bool)\n",
        "    for i in range(B):\n",
        "        start = i * m\n",
        "        pos_mask[i, start : start + m] = True\n",
        "\n",
        "    # exp(logits)\n",
        "    exp_logits = logits.exp()\n",
        "    numerator   = exp_logits.masked_select(pos_mask).view(B, m).sum(dim=1) \n",
        "    denominator = exp_logits.sum(dim=1)                                 \n",
        "    loss = -torch.log(numerator / denominator).mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnet50 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output embedding shape: torch.Size([10, 1024])\n"
          ]
        }
      ],
      "source": [
        "class ResNet50Embedder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet50 backbone producing fixed-size embeddings (e.g., 1024-D).\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, loads ImageNet-pretrained weights.\n",
        "        embedding_dim (int): Dimensionality of the output embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True, embedding_dim: int = 1024, weights=ResNet50_Weights.DEFAULT):\n",
        "        super().__init__()\n",
        "        # Load ResNet50 backbone\n",
        "        if pretrained:\n",
        "            # Use pretrained weights\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "        base_model = resnet50(weights=weights)\n",
        "        # Save the feature dimensionality for projection\n",
        "        in_features = base_model.fc.in_features\n",
        "        # Replace the final fully connected layer with a new one\n",
        "        base_model.fc = nn.Linear(in_features, embedding_dim)\n",
        "        # Initialize the new layer\n",
        "        nn.init.xavier_uniform_(base_model.fc.weight)\n",
        "        self.backbone = base_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: image -> embedding\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Embedding of shape (batch_size, embedding_dim).\n",
        "        \"\"\"\n",
        "        embeddings = self.backbone(x)   # shape: (batch_size, embedding_dim)\n",
        "        return embeddings\n",
        "    \n",
        "    def load_weights(self, path: str):\n",
        "        \"\"\"\n",
        "        Load weights from a file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the weights file.\n",
        "        \"\"\"\n",
        "        state_dict = torch.load(path, weights_only=True)\n",
        "        self.backbone.load_state_dict(state_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
        "# Example: produce 1024-D embeddings\n",
        "encoder = ResNet50Embedder(pretrained=True, embedding_dim=1024).to(device)\n",
        "# summary(model, (3, 224, 224), device=device.type)\n",
        "dummy_input = torch.randn(10, 3, 224, 224, device=device)\n",
        "embed = encoder(dummy_input)\n",
        "print(f\"Output embedding shape: {embed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Validation function ---\n",
        "def validate(model: nn.Module, loader: DataLoader, val_criterion, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, caption_embs in loader:\n",
        "            images, caption_embs = images.to(device), caption_embs.to(device)\n",
        "            img_embs = model(images)                   # (B, D)\n",
        "            # normalize\n",
        "            img_norm = F.normalize(img_embs,    dim=1) # (B, D)\n",
        "            cap_norm = F.normalize(caption_embs, dim=1) # (B, D)\n",
        "            # CosineEmbeddingLoss needs a target of +1 for each pair\n",
        "            targets = torch.ones(images.size(0), device=device)\n",
        "            loss = val_criterion(img_norm, cap_norm, targets)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    model.train()\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-2\n",
        "temperature = 0.07\n",
        "num_epochs = 10\n",
        "\n",
        "# Setup\n",
        "# train_criterion = nn.CrossEntropyLoss()\n",
        "val_criterion = nn.CosineEmbeddingLoss()\n",
        "optimizer = torch.optim.AdamW(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scaler = torch.amp.GradScaler()\n",
        "encoder.train()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "    for images, caption_embs in pbar:\n",
        "        images, caption_embs = images.to(device), caption_embs.to(device)\n",
        "        print(f\"images shape: {images.shape}\")\n",
        "        print(f\"caption_embs shape: {caption_embs.shape}\")\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='mps', dtype=torch.float16):\n",
        "            img_embs = encoder(images)  # (B, D)\n",
        "            img_norm = F.normalize(img_embs, dim=1)      # (B, D)\n",
        "            cap_norm = F.normalize(caption_embs, dim=2)  # (B, m, D)\n",
        "            loss = multi_pos_infonce_loss(img_embs, cap_norm, temperature)\n",
        "\n",
        "        # Backward with mixed precision\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    val_loss = validate(encoder, val_loader, val_criterion, device)\n",
        "    print(f\"Epoch {epoch} Validation CosineEmbeddingLoss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnet152 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /Users/miaoyidi/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 230M/230M [00:04<00:00, 54.1MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
            "             ReLU-81          [-1, 128, 28, 28]               0\n",
            "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
            "             ReLU-84          [-1, 128, 28, 28]               0\n",
            "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-87          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
            "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
            "             ReLU-91          [-1, 128, 28, 28]               0\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "             ReLU-94          [-1, 128, 28, 28]               0\n",
            "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-97          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
            "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
            "            ReLU-101          [-1, 128, 28, 28]               0\n",
            "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
            "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
            "            ReLU-104          [-1, 128, 28, 28]               0\n",
            "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-107          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
            "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
            "            ReLU-111          [-1, 128, 28, 28]               0\n",
            "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
            "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
            "            ReLU-114          [-1, 128, 28, 28]               0\n",
            "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-117          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
            "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
            "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
            "            ReLU-121          [-1, 256, 28, 28]               0\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "            ReLU-124          [-1, 256, 14, 14]               0\n",
            "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
            "            ReLU-143          [-1, 256, 14, 14]               0\n",
            "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
            "            ReLU-146          [-1, 256, 14, 14]               0\n",
            "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-149         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
            "            ReLU-153          [-1, 256, 14, 14]               0\n",
            "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
            "            ReLU-156          [-1, 256, 14, 14]               0\n",
            "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-159         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
            "            ReLU-163          [-1, 256, 14, 14]               0\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "            ReLU-166          [-1, 256, 14, 14]               0\n",
            "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-169         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
            "            ReLU-173          [-1, 256, 14, 14]               0\n",
            "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
            "            ReLU-176          [-1, 256, 14, 14]               0\n",
            "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-179         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "            ReLU-183          [-1, 256, 14, 14]               0\n",
            "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
            "            ReLU-186          [-1, 256, 14, 14]               0\n",
            "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-189         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
            "            ReLU-193          [-1, 256, 14, 14]               0\n",
            "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
            "            ReLU-196          [-1, 256, 14, 14]               0\n",
            "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-199         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
            "            ReLU-203          [-1, 256, 14, 14]               0\n",
            "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
            "            ReLU-206          [-1, 256, 14, 14]               0\n",
            "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-209         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
            "            ReLU-213          [-1, 256, 14, 14]               0\n",
            "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
            "            ReLU-216          [-1, 256, 14, 14]               0\n",
            "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-219         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
            "            ReLU-223          [-1, 256, 14, 14]               0\n",
            "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
            "            ReLU-226          [-1, 256, 14, 14]               0\n",
            "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-229         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
            "            ReLU-233          [-1, 256, 14, 14]               0\n",
            "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
            "            ReLU-236          [-1, 256, 14, 14]               0\n",
            "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-239         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
            "            ReLU-243          [-1, 256, 14, 14]               0\n",
            "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
            "            ReLU-246          [-1, 256, 14, 14]               0\n",
            "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-249         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "            ReLU-253          [-1, 256, 14, 14]               0\n",
            "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
            "            ReLU-256          [-1, 256, 14, 14]               0\n",
            "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-259         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
            "            ReLU-263          [-1, 256, 14, 14]               0\n",
            "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
            "            ReLU-266          [-1, 256, 14, 14]               0\n",
            "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-269         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
            "            ReLU-273          [-1, 256, 14, 14]               0\n",
            "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
            "            ReLU-276          [-1, 256, 14, 14]               0\n",
            "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-279         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
            "            ReLU-283          [-1, 256, 14, 14]               0\n",
            "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
            "            ReLU-286          [-1, 256, 14, 14]               0\n",
            "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-289         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "            ReLU-293          [-1, 256, 14, 14]               0\n",
            "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
            "            ReLU-296          [-1, 256, 14, 14]               0\n",
            "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-299         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
            "            ReLU-303          [-1, 256, 14, 14]               0\n",
            "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
            "            ReLU-306          [-1, 256, 14, 14]               0\n",
            "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-309         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
            "            ReLU-313          [-1, 256, 14, 14]               0\n",
            "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
            "            ReLU-316          [-1, 256, 14, 14]               0\n",
            "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-319         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
            "            ReLU-323          [-1, 256, 14, 14]               0\n",
            "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
            "            ReLU-326          [-1, 256, 14, 14]               0\n",
            "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-329         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
            "            ReLU-333          [-1, 256, 14, 14]               0\n",
            "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
            "            ReLU-336          [-1, 256, 14, 14]               0\n",
            "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-339         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
            "            ReLU-343          [-1, 256, 14, 14]               0\n",
            "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
            "            ReLU-346          [-1, 256, 14, 14]               0\n",
            "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-349         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
            "            ReLU-353          [-1, 256, 14, 14]               0\n",
            "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
            "            ReLU-356          [-1, 256, 14, 14]               0\n",
            "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-359         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
            "            ReLU-363          [-1, 256, 14, 14]               0\n",
            "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
            "            ReLU-366          [-1, 256, 14, 14]               0\n",
            "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-369         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
            "            ReLU-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
            "            ReLU-376          [-1, 256, 14, 14]               0\n",
            "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-379         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
            "            ReLU-383          [-1, 256, 14, 14]               0\n",
            "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
            "            ReLU-386          [-1, 256, 14, 14]               0\n",
            "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-389         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
            "            ReLU-393          [-1, 256, 14, 14]               0\n",
            "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
            "            ReLU-396          [-1, 256, 14, 14]               0\n",
            "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-399         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
            "            ReLU-403          [-1, 256, 14, 14]               0\n",
            "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
            "            ReLU-406          [-1, 256, 14, 14]               0\n",
            "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-409         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
            "            ReLU-413          [-1, 256, 14, 14]               0\n",
            "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
            "            ReLU-416          [-1, 256, 14, 14]               0\n",
            "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-419         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
            "            ReLU-423          [-1, 256, 14, 14]               0\n",
            "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
            "            ReLU-426          [-1, 256, 14, 14]               0\n",
            "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-429         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
            "            ReLU-433          [-1, 256, 14, 14]               0\n",
            "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
            "            ReLU-436          [-1, 256, 14, 14]               0\n",
            "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-439         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
            "            ReLU-443          [-1, 256, 14, 14]               0\n",
            "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
            "            ReLU-446          [-1, 256, 14, 14]               0\n",
            "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-449         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
            "            ReLU-453          [-1, 256, 14, 14]               0\n",
            "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
            "            ReLU-456          [-1, 256, 14, 14]               0\n",
            "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-459         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
            "            ReLU-463          [-1, 256, 14, 14]               0\n",
            "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
            "            ReLU-466          [-1, 256, 14, 14]               0\n",
            "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-469         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
            "            ReLU-473          [-1, 256, 14, 14]               0\n",
            "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
            "            ReLU-476          [-1, 256, 14, 14]               0\n",
            "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-479         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-483          [-1, 512, 14, 14]               0\n",
            "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-486            [-1, 512, 7, 7]               0\n",
            "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-491           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-495            [-1, 512, 7, 7]               0\n",
            "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-498            [-1, 512, 7, 7]               0\n",
            "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-501           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-505            [-1, 512, 7, 7]               0\n",
            "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-508            [-1, 512, 7, 7]               0\n",
            "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-511           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
            "          Linear-514                 [-1, 1024]       2,098,176\n",
            "          ResNet-515                 [-1, 1024]               0\n",
            "================================================================\n",
            "Total params: 60,241,984\n",
            "Trainable params: 60,241,984\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 606.60\n",
            "Params size (MB): 229.80\n",
            "Estimated Total Size (MB): 836.98\n",
            "----------------------------------------------------------------\n",
            "Output embedding shape: torch.Size([10, 1024])\n"
          ]
        }
      ],
      "source": [
        "class ResNet152Embedder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet50 backbone producing fixed-size embeddings (e.g., 1024-D).\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, loads ImageNet-pretrained weights.\n",
        "        embedding_dim (int): Dimensionality of the output embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True, embedding_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        # Load ResNet50 backbone\n",
        "        weights = ResNet152_Weights.DEFAULT if pretrained else None\n",
        "        base_model = resnet152(weights=weights)\n",
        "        # Save the feature dimensionality for projection\n",
        "        in_features = base_model.fc.in_features\n",
        "        # Replace the final fully connected layer with a new one\n",
        "        base_model.fc = nn.Linear(in_features, embedding_dim)\n",
        "        # Initialize the new layer\n",
        "        nn.init.xavier_uniform_(base_model.fc.weight)\n",
        "        self.backbone = base_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: image -> embedding\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Embedding of shape (batch_size, embedding_dim).\n",
        "        \"\"\"\n",
        "        embeddings = self.backbone(x)   # shape: (batch_size, embedding_dim)\n",
        "        return embeddings\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Example: produce 1024-D embeddings\n",
        "model = ResNet152Embedder(pretrained=True, embedding_dim=1024).to(device)\n",
        "summary(model, (3, 224, 224), device=device.type)\n",
        "dummy_input = torch.randn(10, 3, 224, 224, device=device)\n",
        "embed = model(dummy_input)\n",
        "print(f\"Output embedding shape: {embed.shape}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMjePrBFLh06L9uUGDZQMiQ",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vlm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
