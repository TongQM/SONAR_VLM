{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TongQM/SONAR_VLM/blob/main/sonar_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8SKAdaYm_3f"
      },
      "source": [
        "# Install SONAR embeddings and requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqEBmIjJm5kP",
        "outputId": "0c3bda25-3eb0-4ab3-a12f-788ad8b1b639"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: torch\n",
            "Version: 2.6.0\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org/\n",
            "Author: PyTorch Team\n",
            "Author-email: packages@pytorch.org\n",
            "License: BSD-3-Clause\n",
            "Location: /opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, sympy, typing-extensions\n",
            "Required-by: fairseq2n, sonar-space, torchaudio\n"
          ]
        }
      ],
      "source": [
        "# !pip show torch\n",
        "# !pip install fairseq2 --extra-index-url https://fair.pkg.atmeta.com/fairseq2/whl/pt2.6.0/cu124\n",
        "# !pip install sonar-space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VswdE45TnzjT"
      },
      "source": [
        "# Mount Google drive and set work directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRalEcpMn3DA",
        "outputId": "824d3896-9089-4604-ea9d-846207e7b5a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "\n",
        "# drive.mount('/content/gdrive')\n",
        "# os.chdir('/content/gdrive/My Drive/sonar')\n",
        "# !ls annotations/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet50, resnet152, ResNet152_Weights, ResNet50_Weights\n",
        "from torchsummary import summary\n",
        "\n",
        "from sonar.inference_pipelines.text import TextToEmbeddingModelPipeline\n",
        "from sonar.inference_pipelines.text import EmbeddingToTextModelPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = 'mps' if torch.backends.mps.is_available() else device\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFDQCp_p5iu"
      },
      "source": [
        "## Load text2embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original sentences:\n",
            "My name is SONAR.\n",
            "I can embed the sentences into vectorial space.\n",
            "\n",
            "Reconstructed sentences:\n",
            "My name is SONAR.\n",
            "I can embed the sentences into vector space.\n"
          ]
        }
      ],
      "source": [
        "t2vec_model = TextToEmbeddingModelPipeline(encoder=\"text_sonar_basic_encoder\",\n",
        "                                           tokenizer=\"text_sonar_basic_encoder\",\n",
        "                                           device=torch.device('cpu'),\n",
        "                                           dtype=torch.float16)\n",
        "sentences = ['My name is SONAR.', 'I can embed the sentences into vectorial space.']\n",
        "embeddings = t2vec_model.predict(sentences, source_lang=\"eng_Latn\")\n",
        "\n",
        "\n",
        "vec2text_model = EmbeddingToTextModelPipeline(decoder=\"text_sonar_basic_decoder\",\n",
        "                                              tokenizer=\"text_sonar_basic_encoder\",\n",
        "                                              device=torch.device('cpu'),\n",
        "                                              dtype=torch.float16)\n",
        "reconstructed = vec2text_model.predict(embeddings, target_lang=\"eng_Latn\", max_seq_len=512)\n",
        "print(\"Original sentences:\")\n",
        "for sentence in sentences:\n",
        "    print(sentence)\n",
        "print(\"\\nReconstructed sentences:\")\n",
        "for sentence in reconstructed:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgFB2wVJoZKE"
      },
      "source": [
        "## Load train and validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define dataloader with SONAR embeddings as labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [],
      "source": [
        "class COCOCaptionTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns (image_tensor, list_of_captions) for each idx.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_dir, coco_json, transform=None, numcaps=5):\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transform = transform or transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        with open(coco_json) as f:\n",
        "            coco = json.load(f)\n",
        "        # group captions by image_id\n",
        "        caps = defaultdict(list)\n",
        "        for ann in coco[\"annotations\"]:\n",
        "            caps[f\"{ann['image_id']:012d}.jpg\"].append(ann[\"caption\"])\n",
        "        \n",
        "        # For each image, sort the captions and keep only the first 5\n",
        "        for img, captions in caps.items():\n",
        "            caps[img] = sorted(captions)[:numcaps]\n",
        "        \n",
        "        self.samples = sorted((str(self.img_dir / img), caps[img]) \n",
        "                              for img in caps)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, captions = self.samples[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, captions\n",
        "\n",
        "\n",
        "def collate_and_encode(batch, text_encoder, device=\"cuda\"):\n",
        "    \"\"\"\n",
        "    batch: list of (image_tensor, [cap1, cap2, ..., cap5])\n",
        "    text_encoder: any model with .encode(list[str]) → Tensor(n_captions, D)\n",
        "    \"\"\"\n",
        "    # 1) Stack images\n",
        "    imgs = torch.stack([b[0] for b in batch], dim=0)\n",
        "\n",
        "    # 2) Flatten out all captions\n",
        "    all_caps = []\n",
        "    counts   = []\n",
        "    for _, caps in batch:\n",
        "        counts.append(len(caps))\n",
        "        all_caps.extend(caps)\n",
        "\n",
        "    # 3) Run teacher in one shot\n",
        "    #    Assume it returns a torch.Tensor of shape (sum(counts), D)\n",
        "    with torch.no_grad():\n",
        "        embs = text_encoder.predict(all_caps, source_lang=\"eng_Latn\").to(device)\n",
        "        # if it returns numpy, wrap: embs = torch.from_numpy(embs).to(device)\n",
        "\n",
        "    # 4) Split & reduce per sample (e.g. mean)\n",
        "    D = embs.size(1)\n",
        "    labels = []\n",
        "    idx = 0\n",
        "    for n in counts:\n",
        "        chunk = embs[idx: idx+n]      # shape (n, D)\n",
        "        labels.append(chunk)  # → (D,)\n",
        "        idx += n\n",
        "    labels = torch.stack(labels, dim=0)  # → (batch_size, D)\n",
        "\n",
        "    return imgs.to(device), labels\n",
        "\n",
        "\n",
        "# ---- usage ----\n",
        "train_dataset = COCOCaptionTextDataset(\n",
        "    img_dir=\"./data/images/train2017\",\n",
        "    coco_json=\"./data/annotations/annotations/captions_train2017.json\",\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: collate_and_encode(b, t2vec_model, device=\"mps\"),\n",
        ")\n",
        "\n",
        "val_dataset = COCOCaptionTextDataset(\n",
        "    img_dir=\"./data/images/val2017\",\n",
        "    coco_json=\"./data/annotations/annotations/captions_val2017.json\",\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    num_workers=0,\n",
        "    pin_memory=True,\n",
        "    collate_fn=lambda b: collate_and_encode(b, t2vec_model, device=\"mps\"),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define multi-positive InfoNCE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_pos_infonce_loss(img_embs, cap_embs, temperature):\n",
        "    \"\"\"\n",
        "    img_embs: (B, D)  after F.normalize\n",
        "    cap_embs: (B, m, D) after F.normalize\n",
        "    \"\"\"\n",
        "    B, m, D = cap_embs.shape\n",
        "    # flatten captions: (B*m, D)\n",
        "    flat_caps = cap_embs.view(B*m, D)                             \n",
        "    # similarity: (B, B*m)\n",
        "    logits = img_embs @ flat_caps.t() / temperature               \n",
        "\n",
        "    # numerator: sum over each image's m positives\n",
        "    # we know that positives for image i are indices [i*m : i*m + m]\n",
        "    pos_mask = torch.zeros_like(logits, dtype=torch.bool)\n",
        "    for i in range(B):\n",
        "        start = i * m\n",
        "        pos_mask[i, start : start + m] = True\n",
        "\n",
        "    # exp(logits)\n",
        "    exp_logits = logits.exp()\n",
        "    numerator   = exp_logits.masked_select(pos_mask).view(B, m).sum(dim=1) \n",
        "    denominator = exp_logits.sum(dim=1)                                 \n",
        "    loss = -torch.log(numerator / denominator).mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnet50 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output embedding shape: torch.Size([10, 1024])\n"
          ]
        }
      ],
      "source": [
        "class ResNet50Embedder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet50 backbone producing fixed-size embeddings (e.g., 1024-D).\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, loads ImageNet-pretrained weights.\n",
        "        embedding_dim (int): Dimensionality of the output embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True, embedding_dim: int = 1024, weights=ResNet50_Weights.DEFAULT):\n",
        "        super().__init__()\n",
        "        # Load ResNet50 backbone\n",
        "        if pretrained:\n",
        "            # Use pretrained weights\n",
        "            weights = ResNet50_Weights.DEFAULT\n",
        "        base_model = resnet50(weights=weights)\n",
        "        # Save the feature dimensionality for projection\n",
        "        in_features = base_model.fc.in_features\n",
        "        # Replace the final fully connected layer with a new one\n",
        "        base_model.fc = nn.Linear(in_features, embedding_dim)\n",
        "        # Initialize the new layer\n",
        "        nn.init.xavier_uniform_(base_model.fc.weight)\n",
        "        self.backbone = base_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: image -> embedding\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Embedding of shape (batch_size, embedding_dim).\n",
        "        \"\"\"\n",
        "        embeddings = self.backbone(x)   # shape: (batch_size, embedding_dim)\n",
        "        return embeddings\n",
        "    \n",
        "    def load_weights(self, path: str):\n",
        "        \"\"\"\n",
        "        Load weights from a file.\n",
        "\n",
        "        Args:\n",
        "            path (str): Path to the weights file.\n",
        "        \"\"\"\n",
        "        state_dict = torch.load(path, weights_only=True)\n",
        "        self.backbone.load_state_dict(state_dict)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else device)\n",
        "# Example: produce 1024-D embeddings\n",
        "encoder = ResNet50Embedder(pretrained=True, embedding_dim=1024).to(device)\n",
        "# summary(model, (3, 224, 224), device=device.type)\n",
        "dummy_input = torch.randn(10, 3, 224, 224, device=device)\n",
        "embed = encoder(dummy_input)\n",
        "print(f\"Output embedding shape: {embed.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Validation function ---\n",
        "def validate(model: nn.Module, loader: DataLoader, val_criterion, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for images, caption_embs in loader:\n",
        "            images, caption_embs = images.to(device), caption_embs.to(device)\n",
        "            img_embs = model(images)                   # (B, D)\n",
        "            # normalize\n",
        "            img_norm = F.normalize(img_embs,    dim=1) # (B, D)\n",
        "            cap_norm = F.normalize(caption_embs, dim=1) # (B, D)\n",
        "            # CosineEmbeddingLoss needs a target of +1 for each pair\n",
        "            targets = torch.ones(images.size(0), device=device)\n",
        "            loss = val_criterion(img_norm, cap_norm, targets)\n",
        "            total_loss += loss.item() * images.size(0)\n",
        "    avg_loss = total_loss / len(loader.dataset)\n",
        "    model.train()\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n",
            "Epoch 1/10:   1%|          | 32/3697 [05:07<9:47:50,  9.62s/it, train_loss=nan]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[211]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m     15\u001b[39m     pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption_embs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption_embs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaption_embs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(b)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# ---- usage ----\u001b[39;00m\n\u001b[32m     73\u001b[39m train_dataset = COCOCaptionTextDataset(\n\u001b[32m     74\u001b[39m     img_dir=\u001b[33m\"\u001b[39m\u001b[33m./data/images/train2017\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     75\u001b[39m     coco_json=\u001b[33m\"\u001b[39m\u001b[33m./data/annotations/annotations/captions_train2017.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     76\u001b[39m )\n\u001b[32m     77\u001b[39m train_loader = DataLoader(\n\u001b[32m     78\u001b[39m     train_dataset,\n\u001b[32m     79\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m     80\u001b[39m     num_workers=\u001b[32m0\u001b[39m,\n\u001b[32m     81\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m b: \u001b[43mcollate_and_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2vec_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m     83\u001b[39m )\n\u001b[32m     85\u001b[39m val_dataset = COCOCaptionTextDataset(\n\u001b[32m     86\u001b[39m     img_dir=\u001b[33m\"\u001b[39m\u001b[33m./data/images/val2017\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m     coco_json=\u001b[33m\"\u001b[39m\u001b[33m./data/annotations/annotations/captions_val2017.json\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     88\u001b[39m )\n\u001b[32m     89\u001b[39m val_loader = DataLoader(\n\u001b[32m     90\u001b[39m     val_dataset,\n\u001b[32m     91\u001b[39m     batch_size=\u001b[32m32\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     collate_fn=\u001b[38;5;28;01mlambda\u001b[39;00m b: collate_and_encode(b, t2vec_model, device=\u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     95\u001b[39m )\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[209]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mcollate_and_encode\u001b[39m\u001b[34m(batch, text_encoder, device)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# 3) Run teacher in one shot\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#    Assume it returns a torch.Tensor of shape (sum(counts), D)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     embs = \u001b[43mtext_encoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_caps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_lang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meng_Latn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# if it returns numpy, wrap: embs = torch.from_numpy(embs).to(device)\u001b[39;00m\n\u001b[32m     58\u001b[39m \n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# 4) Split & reduce per sample (e.g. mean)\u001b[39;00m\n\u001b[32m     60\u001b[39m D = embs.size(\u001b[32m1\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/sonar/inference_pipelines/text.py:256\u001b[39m, in \u001b[36mTextToEmbeddingModelPipeline.predict\u001b[39m\u001b[34m(self, input, source_lang, batch_size, batch_max_tokens, max_seq_len, progress_bar, target_device)\u001b[39m\n\u001b[32m    249\u001b[39m     pipeline = add_progress_bar(\n\u001b[32m    250\u001b[39m         pipeline,\n\u001b[32m    251\u001b[39m         inputs=\u001b[38;5;28minput\u001b[39m,\n\u001b[32m    252\u001b[39m         batch_size=batch_size \u001b[38;5;28;01mif\u001b[39;00m batch_max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    253\u001b[39m     )\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m precision_context(\u001b[38;5;28mself\u001b[39m.model.dtype):\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     results: List[torch.Tensor] = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28miter\u001b[39m(pipeline))\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_truncated:\n\u001b[32m    259\u001b[39m     warnings.warn(\n\u001b[32m    260\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_truncated\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m input tensors for SONAR text encoder, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    261\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mthe length was truncated to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m elements.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    262\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/sonar/models/sonar_text/model.py:134\u001b[39m, in \u001b[36mSonarTextTransformerEncoderModel.forward\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: SequenceBatch) -> SonarEncoderOutput:\n\u001b[32m    132\u001b[39m     embed_seqs, padding_mask = \u001b[38;5;28mself\u001b[39m.encoder_frontend(batch.seqs, batch.padding_mask)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     encoded_seqs, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed_seqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layer_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    137\u001b[39m         encoded_seqs = \u001b[38;5;28mself\u001b[39m.layer_norm(encoded_seqs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/fairseq2/nn/transformer/_encoder.py:218\u001b[39m, in \u001b[36mStandardTransformerEncoder.forward\u001b[39m\u001b[34m(self, seqs, padding_mask)\u001b[39m\n\u001b[32m    213\u001b[39m     self_attn_mask = \u001b[38;5;28mself\u001b[39m.self_attn_mask_factory(\n\u001b[32m    214\u001b[39m         seqs, keys=seqs, training=\u001b[38;5;28mself\u001b[39m.training\n\u001b[32m    215\u001b[39m     )\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, (layer, drop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m._drop_iter()):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     layer_output, layer_padding_mask = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m drop:\n\u001b[32m    221\u001b[39m         seqs = _record_drop_for_backward(seqs, layer_output)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/fairseq2/nn/transformer/_encoder_layer.py:187\u001b[39m, in \u001b[36mStandardTransformerEncoderLayer.forward\u001b[39m\u001b[34m(self, seqs, padding_mask, self_attn_mask)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    182\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m     self_attn_mask: AttentionMask | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    186\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[Tensor, PaddingMask | \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     seqs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_self_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    189\u001b[39m     seqs = \u001b[38;5;28mself\u001b[39m._forward_ffn(seqs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m seqs, padding_mask\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/fairseq2/nn/transformer/_encoder_layer.py:204\u001b[39m, in \u001b[36mStandardTransformerEncoderLayer._forward_self_attn\u001b[39m\u001b[34m(self, seqs, padding_mask, self_attn_mask)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm_order != TransformerNormOrder.POST:\n\u001b[32m    202\u001b[39m     seqs = \u001b[38;5;28mself\u001b[39m.self_attn_layer_norm(seqs)\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m seqs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.self_attn_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    214\u001b[39m     seqs = \u001b[38;5;28mself\u001b[39m.self_attn_norm(seqs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/fairseq2/nn/transformer/_multihead_attention.py:514\u001b[39m, in \u001b[36mStandardMultiheadAttention.forward\u001b[39m\u001b[34m(self, seqs, padding_mask, keys, key_padding_mask, values, attn_mask, state_bag)\u001b[39m\n\u001b[32m    511\u001b[39m attn = attn.flatten(\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m)\n\u001b[32m    513\u001b[39m \u001b[38;5;66;03m# (N, S, V_proj) -> (N, S, M)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m attn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/anaconda3/envs/vlm/lib/python3.11/site-packages/fairseq2/nn/_projection.py:126\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-2\n",
        "temperature = 0.07\n",
        "num_epochs = 10\n",
        "\n",
        "# Setup\n",
        "# train_criterion = nn.CrossEntropyLoss()\n",
        "val_criterion = nn.CosineEmbeddingLoss()\n",
        "optimizer = torch.optim.AdamW(encoder.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scaler = torch.amp.GradScaler()\n",
        "encoder.train()\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "    for images, caption_embs in pbar:\n",
        "        images, caption_embs = images.to(device), caption_embs.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='mps', dtype=torch.float16):\n",
        "            img_embs = encoder(images)  # (B, D)\n",
        "            img_norm = F.normalize(img_embs, dim=1)      # (B, D)\n",
        "            cap_norm = F.normalize(caption_embs, dim=2)  # (B, m, D)\n",
        "            loss = multi_pos_infonce_loss(img_embs, cap_norm, temperature)\n",
        "\n",
        "        # Backward with mixed precision\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        pbar.set_postfix(train_loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "    val_loss = validate(encoder, val_loader, val_criterion, device)\n",
        "    print(f\"Epoch {epoch} Validation CosineEmbeddingLoss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resnet152 Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /Users/miaoyidi/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n",
            "100%|██████████| 230M/230M [00:04<00:00, 54.1MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
            "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
            "             ReLU-15          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
            "             ReLU-19           [-1, 64, 56, 56]               0\n",
            "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
            "             ReLU-22           [-1, 64, 56, 56]               0\n",
            "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
            "             ReLU-25          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
            "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
            "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
            "             ReLU-29           [-1, 64, 56, 56]               0\n",
            "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
            "             ReLU-32           [-1, 64, 56, 56]               0\n",
            "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
            "             ReLU-35          [-1, 256, 56, 56]               0\n",
            "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
            "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
            "             ReLU-39          [-1, 128, 56, 56]               0\n",
            "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
            "             ReLU-42          [-1, 128, 28, 28]               0\n",
            "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
            "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-47          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
            "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
            "             ReLU-51          [-1, 128, 28, 28]               0\n",
            "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
            "             ReLU-54          [-1, 128, 28, 28]               0\n",
            "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-57          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
            "             ReLU-61          [-1, 128, 28, 28]               0\n",
            "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
            "             ReLU-64          [-1, 128, 28, 28]               0\n",
            "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-67          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
            "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
            "             ReLU-71          [-1, 128, 28, 28]               0\n",
            "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
            "             ReLU-74          [-1, 128, 28, 28]               0\n",
            "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-77          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
            "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
            "             ReLU-81          [-1, 128, 28, 28]               0\n",
            "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
            "             ReLU-84          [-1, 128, 28, 28]               0\n",
            "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-87          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
            "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
            "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
            "             ReLU-91          [-1, 128, 28, 28]               0\n",
            "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
            "             ReLU-94          [-1, 128, 28, 28]               0\n",
            "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
            "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
            "             ReLU-97          [-1, 512, 28, 28]               0\n",
            "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
            "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
            "            ReLU-101          [-1, 128, 28, 28]               0\n",
            "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
            "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
            "            ReLU-104          [-1, 128, 28, 28]               0\n",
            "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-107          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
            "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
            "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
            "            ReLU-111          [-1, 128, 28, 28]               0\n",
            "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
            "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
            "            ReLU-114          [-1, 128, 28, 28]               0\n",
            "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
            "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
            "            ReLU-117          [-1, 512, 28, 28]               0\n",
            "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
            "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
            "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
            "            ReLU-121          [-1, 256, 28, 28]               0\n",
            "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
            "            ReLU-124          [-1, 256, 14, 14]               0\n",
            "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-129         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
            "            ReLU-133          [-1, 256, 14, 14]               0\n",
            "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
            "            ReLU-136          [-1, 256, 14, 14]               0\n",
            "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-139         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
            "            ReLU-143          [-1, 256, 14, 14]               0\n",
            "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
            "            ReLU-146          [-1, 256, 14, 14]               0\n",
            "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-149         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
            "            ReLU-153          [-1, 256, 14, 14]               0\n",
            "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
            "            ReLU-156          [-1, 256, 14, 14]               0\n",
            "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-159         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
            "            ReLU-163          [-1, 256, 14, 14]               0\n",
            "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
            "            ReLU-166          [-1, 256, 14, 14]               0\n",
            "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-169         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
            "            ReLU-173          [-1, 256, 14, 14]               0\n",
            "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
            "            ReLU-176          [-1, 256, 14, 14]               0\n",
            "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-179         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
            "            ReLU-183          [-1, 256, 14, 14]               0\n",
            "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
            "            ReLU-186          [-1, 256, 14, 14]               0\n",
            "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-189         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
            "            ReLU-193          [-1, 256, 14, 14]               0\n",
            "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
            "            ReLU-196          [-1, 256, 14, 14]               0\n",
            "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-199         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
            "            ReLU-203          [-1, 256, 14, 14]               0\n",
            "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
            "            ReLU-206          [-1, 256, 14, 14]               0\n",
            "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-209         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
            "            ReLU-213          [-1, 256, 14, 14]               0\n",
            "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
            "            ReLU-216          [-1, 256, 14, 14]               0\n",
            "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-219         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
            "            ReLU-223          [-1, 256, 14, 14]               0\n",
            "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
            "            ReLU-226          [-1, 256, 14, 14]               0\n",
            "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-229         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
            "            ReLU-233          [-1, 256, 14, 14]               0\n",
            "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
            "            ReLU-236          [-1, 256, 14, 14]               0\n",
            "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-239         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
            "            ReLU-243          [-1, 256, 14, 14]               0\n",
            "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
            "            ReLU-246          [-1, 256, 14, 14]               0\n",
            "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-249         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
            "            ReLU-253          [-1, 256, 14, 14]               0\n",
            "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
            "            ReLU-256          [-1, 256, 14, 14]               0\n",
            "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-259         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
            "            ReLU-263          [-1, 256, 14, 14]               0\n",
            "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
            "            ReLU-266          [-1, 256, 14, 14]               0\n",
            "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-269         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
            "            ReLU-273          [-1, 256, 14, 14]               0\n",
            "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
            "            ReLU-276          [-1, 256, 14, 14]               0\n",
            "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-279         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
            "            ReLU-283          [-1, 256, 14, 14]               0\n",
            "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
            "            ReLU-286          [-1, 256, 14, 14]               0\n",
            "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-289         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
            "            ReLU-293          [-1, 256, 14, 14]               0\n",
            "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
            "            ReLU-296          [-1, 256, 14, 14]               0\n",
            "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-299         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
            "            ReLU-303          [-1, 256, 14, 14]               0\n",
            "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
            "            ReLU-306          [-1, 256, 14, 14]               0\n",
            "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-309         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
            "            ReLU-313          [-1, 256, 14, 14]               0\n",
            "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
            "            ReLU-316          [-1, 256, 14, 14]               0\n",
            "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-319         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
            "            ReLU-323          [-1, 256, 14, 14]               0\n",
            "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
            "            ReLU-326          [-1, 256, 14, 14]               0\n",
            "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-329         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
            "            ReLU-333          [-1, 256, 14, 14]               0\n",
            "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
            "            ReLU-336          [-1, 256, 14, 14]               0\n",
            "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-339         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
            "            ReLU-343          [-1, 256, 14, 14]               0\n",
            "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
            "            ReLU-346          [-1, 256, 14, 14]               0\n",
            "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-349         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
            "            ReLU-353          [-1, 256, 14, 14]               0\n",
            "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
            "            ReLU-356          [-1, 256, 14, 14]               0\n",
            "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-359         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
            "            ReLU-363          [-1, 256, 14, 14]               0\n",
            "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
            "            ReLU-366          [-1, 256, 14, 14]               0\n",
            "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-369         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
            "            ReLU-373          [-1, 256, 14, 14]               0\n",
            "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
            "            ReLU-376          [-1, 256, 14, 14]               0\n",
            "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-379         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
            "            ReLU-383          [-1, 256, 14, 14]               0\n",
            "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
            "            ReLU-386          [-1, 256, 14, 14]               0\n",
            "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-389         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
            "            ReLU-393          [-1, 256, 14, 14]               0\n",
            "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
            "            ReLU-396          [-1, 256, 14, 14]               0\n",
            "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-399         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
            "            ReLU-403          [-1, 256, 14, 14]               0\n",
            "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
            "            ReLU-406          [-1, 256, 14, 14]               0\n",
            "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-409         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
            "            ReLU-413          [-1, 256, 14, 14]               0\n",
            "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
            "            ReLU-416          [-1, 256, 14, 14]               0\n",
            "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-419         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
            "            ReLU-423          [-1, 256, 14, 14]               0\n",
            "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
            "            ReLU-426          [-1, 256, 14, 14]               0\n",
            "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-429         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
            "            ReLU-433          [-1, 256, 14, 14]               0\n",
            "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
            "            ReLU-436          [-1, 256, 14, 14]               0\n",
            "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-439         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
            "            ReLU-443          [-1, 256, 14, 14]               0\n",
            "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
            "            ReLU-446          [-1, 256, 14, 14]               0\n",
            "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-449         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
            "            ReLU-453          [-1, 256, 14, 14]               0\n",
            "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
            "            ReLU-456          [-1, 256, 14, 14]               0\n",
            "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-459         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
            "            ReLU-463          [-1, 256, 14, 14]               0\n",
            "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
            "            ReLU-466          [-1, 256, 14, 14]               0\n",
            "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-469         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
            "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
            "            ReLU-473          [-1, 256, 14, 14]               0\n",
            "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
            "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
            "            ReLU-476          [-1, 256, 14, 14]               0\n",
            "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
            "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
            "            ReLU-479         [-1, 1024, 14, 14]               0\n",
            "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
            "            ReLU-483          [-1, 512, 14, 14]               0\n",
            "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-486            [-1, 512, 7, 7]               0\n",
            "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
            "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-491           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-495            [-1, 512, 7, 7]               0\n",
            "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-498            [-1, 512, 7, 7]               0\n",
            "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-501           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
            "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-505            [-1, 512, 7, 7]               0\n",
            "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
            "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
            "            ReLU-508            [-1, 512, 7, 7]               0\n",
            "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
            "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
            "            ReLU-511           [-1, 2048, 7, 7]               0\n",
            "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
            "          Linear-514                 [-1, 1024]       2,098,176\n",
            "          ResNet-515                 [-1, 1024]               0\n",
            "================================================================\n",
            "Total params: 60,241,984\n",
            "Trainable params: 60,241,984\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 606.60\n",
            "Params size (MB): 229.80\n",
            "Estimated Total Size (MB): 836.98\n",
            "----------------------------------------------------------------\n",
            "Output embedding shape: torch.Size([10, 1024])\n"
          ]
        }
      ],
      "source": [
        "class ResNet152Embedder(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet50 backbone producing fixed-size embeddings (e.g., 1024-D).\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, loads ImageNet-pretrained weights.\n",
        "        embedding_dim (int): Dimensionality of the output embedding.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained: bool = True, embedding_dim: int = 1024):\n",
        "        super().__init__()\n",
        "        # Load ResNet50 backbone\n",
        "        weights = ResNet152_Weights.DEFAULT if pretrained else None\n",
        "        base_model = resnet152(weights=weights)\n",
        "        # Save the feature dimensionality for projection\n",
        "        in_features = base_model.fc.in_features\n",
        "        # Replace the final fully connected layer with a new one\n",
        "        base_model.fc = nn.Linear(in_features, embedding_dim)\n",
        "        # Initialize the new layer\n",
        "        nn.init.xavier_uniform_(base_model.fc.weight)\n",
        "        self.backbone = base_model\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass: image -> embedding\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, 3, H, W).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Embedding of shape (batch_size, embedding_dim).\n",
        "        \"\"\"\n",
        "        embeddings = self.backbone(x)   # shape: (batch_size, embedding_dim)\n",
        "        return embeddings\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Example: produce 1024-D embeddings\n",
        "model = ResNet152Embedder(pretrained=True, embedding_dim=1024).to(device)\n",
        "summary(model, (3, 224, 224), device=device.type)\n",
        "dummy_input = torch.randn(10, 3, 224, 224, device=device)\n",
        "embed = model(dummy_input)\n",
        "print(f\"Output embedding shape: {embed.shape}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMjePrBFLh06L9uUGDZQMiQ",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "vlm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
